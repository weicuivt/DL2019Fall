{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "ECE-CS 6524 - Assignment 2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weicuivt/DL2019Fall/blob/master/HW2/ECE_CS_6524_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyTH9M3GLDDu",
        "colab_type": "text"
      },
      "source": [
        "# ECE-6524 / CS-6524 Deep Learning\n",
        "# Assignment 2\n",
        "\n",
        "In this assignment, **you need to complete the following three sectoins**:\n",
        "1. PyTorch Basics\n",
        "    - Toy example with PyTorch\n",
        "2. Image Classification with PyTorch\n",
        "    - Implement a simple MLP network for image classification\n",
        "    - Implement a convolutional network for image classification\n",
        "    - Experiment with different numbers of layers and optimizers\n",
        "    - Push the performance of your CNN\n",
        "\n",
        "This assignment is inspired and adopted from the official PyTorch tutorial.\n",
        "## Submission guideline\n",
        "\n",
        "1. Click the Save button at the top of the Jupyter Notebook.\n",
        "2. Please make sure to have entered your Virginia Tech PID below.\n",
        "3. Select Cell -> All Output -> Clear. This will clear all the outputs from all cells (but will keep the content of cells).\n",
        "4. Select Cell -> Run All. This will run all the cells in order.\n",
        "5. Once you've rerun everything, select File -> Download as -> PDF via LaTeX\n",
        "6. Look at the PDF file and make sure all your solutions are displayed correctly there. \n",
        "7. Zip the all the files along with this notebook (Please don't include the data)\n",
        "8. Name your PDF file as Assignment2_[YOUR ID NUMBER].\n",
        "9. Submit your zipped file and the PDF **INDEPENDENTLY**.\n",
        "\n",
        "**While you are encouraged to discuss with your peers, <span style=\"color:blue\">all work submitted is expected to be your own.</span> <span style=\"color:red\">If you use any information from other resources (e.g. online materials), you are required to cite it below you VT PID. Any violation will result in a 0 mark for the assignment.</span>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVIPVc7DLDDx",
        "colab_type": "text"
      },
      "source": [
        "### Please Write Your VT PID Here: 905808996\n",
        "### Reference (if any):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLVokOpHLDDy",
        "colab_type": "text"
      },
      "source": [
        "In this homework, you would need to use **Python 3.6+** along with the following packages:\n",
        "```\n",
        "1. pytorch 1.2\n",
        "2. torchvision\n",
        "3. numpy\n",
        "4. matplotlib\n",
        "```\n",
        "To install pytorch, please follow the instructions on the [Official website](https://pytorch.org/). In addition, the [official document](https://pytorch.org/docs/stable/) could be very helpful when you want to find certain functionalities. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk075vWFjtiK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "42fe6334-01eb-4c9a-c397-5e5d95a20778"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/Colab Notebooks/Assignment_2\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/Colab Notebooks/Assignment_2\n",
            " Assignment2_905808996.zip   data  'ECE-CS 6524 - Assignment 2.ipynb'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clwk7m-qLDDz",
        "colab_type": "text"
      },
      "source": [
        "# Section 1. PyTorch Basics [30 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpXg2lBiLDD0",
        "colab_type": "text"
      },
      "source": [
        "Simply put, PyTorch is a **Tensor** library like Numpy. These two libraries similarly provide useful and efficient APIs for you to deal with your tensor data. What really differentiate PyTorch from Numpy are the following two features:\n",
        "1. Numerical operations that can **run on GPUs** (more than 10x speedup)\n",
        "2. Automatic differentiation for building and training neural networks\n",
        "\n",
        "In this section, we will walk through some simple example, and see how the automatic differentiation functionality can make your life much easier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8r9ki1rLDD1",
        "colab_type": "text"
      },
      "source": [
        "## 1.1. Automatic Differentiation\n",
        "Gradient descent is the driving force of the deep learning field. In the lectures and assignment 1, we learned how to derive the gradient for a given function, and implement methods for calculating and performing gradient descents. We also see how we can manually implement the backward and forward functions for the simple NN example. While implementing these functions may not be a big deal for a small network, it may get very nasty when we want to build something with tens of hundreds of layers.\n",
        "\n",
        "In PyTorch (as well as other major deep learning libraries), we can use autograd ([automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)) to handle the tedious computation of backward passes. When doing forward passes with autograd, we are essentially defining a **computational graph**, while the nodes in the graph are **tensors**, the edges are the functions that produce output tensors (e.g. ReLU, Linear, Convolutional Layer) given the input tensors. To do backpropagation, we can simply backtrack through this graph to compute gradients. \n",
        "\n",
        "This may sound a little bit abstract, so let's take a look at the example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RlBk-mULDD2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "655518d2-7f24-41a6-a22f-da2b5049ff91"
      },
      "source": [
        "import torch # import pytorch.\n",
        "\n",
        "target = 10.\n",
        "\n",
        "# create a matrix of size 2x2. Each with value draws from standard normal distribution.\n",
        "x = torch.randn(2, 2, requires_grad=True) \n",
        "y = torch.randn(2, 2, requires_grad=True)\n",
        "\n",
        "a = x + y\n",
        "b = a.sum()\n",
        "loss = b - target\n",
        "\n",
        "# print out each tensor:\n",
        "print(x)\n",
        "print(y)\n",
        "print(a)\n",
        "print(b)\n",
        "print(loss)\n",
        "\n",
        "print(\"-----gradient-----\")\n",
        "print(x.grad)\n",
        "print(y.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.6025, -0.6463],\n",
            "        [-0.1990,  0.5098]], requires_grad=True)\n",
            "tensor([[-0.4679, -0.8833],\n",
            "        [-1.7347, -0.5699]], requires_grad=True)\n",
            "tensor([[-1.0704, -1.5295],\n",
            "        [-1.9337, -0.0601]], grad_fn=<AddBackward0>)\n",
            "tensor(-4.5937, grad_fn=<SumBackward0>)\n",
            "tensor(-14.5937, grad_fn=<SubBackward0>)\n",
            "-----gradient-----\n",
            "None\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diO6ImgPLDD6",
        "colab_type": "text"
      },
      "source": [
        "In the above example, we have seen a few things:\n",
        "1. `requires_grad` flag: If false, we can safely exclude this tensor (and its subgraph) from gradient computation and therefore increase efficiency.\n",
        "2. `grad_fn`: we can see that once an operation is done to a tensor, the output tensor is bound to a backward function associated to the operation. In this case, we have Add, Sum, and Sub.\n",
        "\n",
        "However, even if we set `requires_grad=True`, we still don't have gradient for `x` and `y`. This is because that we haven't performed the backpropagation yet. So let's do it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cro5AVHeLDD7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "10354d25-0a9a-4418-bd4e-f5c875dfb788"
      },
      "source": [
        "# perform backpropagation from this \"node\"\n",
        "loss.backward()\n",
        "print('-----gradient-----')\n",
        "print(x.grad)\n",
        "print(y.grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----gradient-----\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXydmGAYLDD9",
        "colab_type": "text"
      },
      "source": [
        "Great, seems like we can perform gradient descent without writing backwards function! Now, let's see a simple toy example on how we can fit some weights `w1` and `w2` with random input `x` and target `y`: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5dyXijGLDD-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "6c127050-41cc-4f2c-a4b0-53e8a56e8435"
      },
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random Tensors to hold input and outputs.\n",
        "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
        "# with respect to these Tensors during the backward pass.\n",
        "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
        "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
        "\n",
        "# Create random Tensors for weights.\n",
        "# Setting requires_grad=True indicates that we want to compute gradients with\n",
        "# respect to these Tensors during the backward pass.\n",
        "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
        "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(500):\n",
        "    # Forward pass: compute predicted y using operations on Tensors; these\n",
        "    # are exactly the same operations we used to compute the forward pass using\n",
        "    # Tensors, but we do not need to keep references to intermediate values since\n",
        "    # we are not implementing the backward pass by hand.\n",
        "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
        "\n",
        "    # Compute and print loss using operations on Tensors.\n",
        "    # Now loss is a Tensor of shape (1,)\n",
        "    # loss.item() gets the scalar value held in the loss.\n",
        "    loss = (y_pred - y).pow(2).sum()\n",
        "    if t % 100 == 99:\n",
        "        print(f'iteration {t}: {loss.item()}')\n",
        "    # Use autograd to compute the backward pass. This call will compute the\n",
        "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
        "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
        "    # of the loss with respect to w1 and w2 respectively.\n",
        "    loss.backward()\n",
        "\n",
        "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
        "    # because weights have requires_grad=True, but we don't need to track this\n",
        "    # in autograd.  (because we don't need the gradient for the operation \n",
        "    # learning_rate * w1.grad)\n",
        "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
        "    # Recall that tensor.data gives a tensor that shares the storage with\n",
        "    # tensor, but doesn't track history.\n",
        "    # You can also use torch.optim.SGD to achieve this.\n",
        "    with torch.no_grad():\n",
        "        w1 -= learning_rate * w1.grad\n",
        "        w2 -= learning_rate * w2.grad\n",
        "\n",
        "        # Manually zero the gradients after updating weights\n",
        "        w1.grad.zero_()\n",
        "        w2.grad.zero_()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 99: 871.6419677734375\n",
            "iteration 199: 9.039835929870605\n",
            "iteration 299: 0.12880882620811462\n",
            "iteration 399: 0.002259192056953907\n",
            "iteration 499: 0.00016256274830084294\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kn7GqA3LDEB",
        "colab_type": "text"
      },
      "source": [
        "## 1.2. `nn` Module\n",
        "Computational graphs and autograd are a very powerful paradigm for defining complex operators and automatically taking derivatives; however for large neural networks raw autograd can be a bit too low-level.\n",
        "\n",
        "When building neural networks we frequently think of arranging the computation into layers, some of which have learnable parameters which will be optimized during learning.\n",
        "\n",
        "In PyTorch, the nn package serves this purpose. The nn package defines a set of Modules, which are roughly equivalent to neural network layers. A Module receives input Tensors and computes output Tensors, but may also hold internal state such as Tensors containing learnable parameters. The nn package also defines a set of useful loss functions that are commonly used when training neural networks.\n",
        "\n",
        "Now, let's see how our simple NN could be implemented using the nn module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeoJOYfaLDEB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9f52d9a2-2798-4368-a8dc-44e85f0ae7e8"
      },
      "source": [
        "import torch.nn as nn\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random Tensors to hold inputs and outputs\n",
        "x = torch.randn(N, D_in)\n",
        "y = torch.randn(N, D_out)\n",
        "\n",
        "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
        "# is a Module which contains other Modules, and applies them in sequence to\n",
        "# produce its output. Each Linear Module computes output from input using a\n",
        "# linear function, and holds internal Tensors for its weight and bias.\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(D_in, H),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(H, D_out),\n",
        ")\n",
        "\n",
        "# The nn package also contains definitions of popular loss functions; in this\n",
        "# case we will use Mean Squared Error (MSE) as our loss function.\n",
        "loss_fn = nn.MSELoss(reduction='sum')\n",
        "\n",
        "learning_rate = 1e-4\n",
        "for t in range(500):\n",
        "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
        "    # override the __call__ operator so you can call them like functions. When\n",
        "    # doing so you pass a Tensor of input data to the Module and it produces\n",
        "    # a Tensor of output data.\n",
        "    y_pred = model(x)\n",
        "\n",
        "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
        "    # values of y, and the loss function returns a Tensor containing the\n",
        "    # loss.\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(f'iteration {t}: {loss.item()}')\n",
        "\n",
        "    # Zero the gradients before running the backward pass.\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
        "    # parameters of the model. Internally, the parameters of each Module are stored\n",
        "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
        "    # all learnable parameters in the model.\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
        "    # we can access its gradients like we did before.\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= learning_rate * param.grad"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 99: 2.681800127029419\n",
            "iteration 199: 0.0444968044757843\n",
            "iteration 299: 0.0013897938188165426\n",
            "iteration 399: 5.530680937226862e-05\n",
            "iteration 499: 2.3638890525035094e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5Z1TyQ7LDEE",
        "colab_type": "text"
      },
      "source": [
        "So far, we have been updating the model parameters manually with `torch.no_grad()`. However, if we want to use optimization algorithms other than SGD, it might get a bit nasty to do it manually. Instead of manually doing this, we can use `optim` pacakge to help optimize our model: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw8M5VI2fSCx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "b86de974-0b3a-4816-8725-6dc066263466"
      },
      "source": [
        "x = torch.randn(5,3)\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.7430, -0.0024,  0.2844],\n",
            "        [ 1.9516,  0.5675,  2.3156],\n",
            "        [ 0.6972, -1.4555,  2.1397],\n",
            "        [-1.6965,  0.0881,  0.1323],\n",
            "        [ 1.5326,  0.6526,  0.8175]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8MMlExrLDEF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "b1a4d155-7ed6-4710-9dea-9044213a4fd6"
      },
      "source": [
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random Tensors to hold inputs and outputs\n",
        "x = torch.randn(N, D_in)\n",
        "y = torch.randn(N, D_out)\n",
        "\n",
        "# Use the nn package to define our model and loss function.\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(D_in, H),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(H, D_out),\n",
        ")\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "# Use the optim package to define an Optimizer that will update the weights of\n",
        "# the model for us. \n",
        "learning_rate = 1e-4\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "for t in range(500):\n",
        "    # Forward pass: compute predicted y by passing x to the model.\n",
        "    y_pred = model(x)\n",
        "\n",
        "    # Compute and print loss.\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(f'iteration {t}: {loss.item()}')\n",
        "\n",
        "    # Before the backward pass, use the optimizer object to zero all of the\n",
        "    # gradients for the variables it will update (which are the learnable\n",
        "    # weights of the model). This is because by default, gradients are\n",
        "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
        "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backward pass: compute gradient of the loss with respect to model\n",
        "    # parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # Calling the step function on an Optimizer makes an update to its\n",
        "    # parameters\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 99: 2.9050896167755127\n",
            "iteration 199: 0.05743909254670143\n",
            "iteration 299: 0.0017912393668666482\n",
            "iteration 399: 6.933413533261046e-05\n",
            "iteration 499: 3.028186938536237e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwq7ibZELDEH",
        "colab_type": "text"
      },
      "source": [
        "Sometimes you will want to specify models that are more complex than a sequence of existing Modules; for these cases you can define your own Modules by subclassing nn.Module and defining a forward which receives input Tensors and produces output Tensors using other modules or other autograd operations on Tensors.\n",
        "\n",
        "For example, we can implement our 2-layer simple NN as the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3OTxg94LDEI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f0d6a0e3-9d48-4b8d-e6ed-994a699c80a1"
      },
      "source": [
        "class TwoLayerNet(nn.Module):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
        "        member variables.\n",
        "        \"\"\"\n",
        "        super(TwoLayerNet, self).__init__()\n",
        "        self.linear1 = nn.Linear(D_in, H)\n",
        "        self.linear2 = nn.Linear(H, D_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Tensor of input data and we must return\n",
        "        a Tensor of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Tensors.\n",
        "        \"\"\"\n",
        "        h_relu = self.linear1(x).clamp(min=0)\n",
        "        y_pred = self.linear2(h_relu)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random Tensors to hold inputs and outputs\n",
        "x = torch.randn(N, D_in)\n",
        "y = torch.randn(N, D_out)\n",
        "\n",
        "# Construct our model by instantiating the class defined above\n",
        "model = TwoLayerNet(D_in, H, D_out)\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
        "for t in range(500):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(f'iteration {t}: {loss.item()}')\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 99: 2.4881346225738525\n",
            "iteration 199: 0.035798005759716034\n",
            "iteration 299: 0.0009966156212612987\n",
            "iteration 399: 3.5246514016762376e-05\n",
            "iteration 499: 1.437473542864609e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9C7w4U6LDEL",
        "colab_type": "text"
      },
      "source": [
        "## 1.3. Warm-up: Two-moon datasets [30 pts]\n",
        "Now, let's use PyTorch to solve some synthetic datasets. In previous assignment, we have to write some codes to create training batches. Again, this can also be done with PyTorch `DataLoader`. The `DataLoader` utilizes parallel workers to read and prepare batches for you, which can greatly speedup the code when your time bottleneck is on file I/O.\n",
        "\n",
        "Here, we show a simple example that can create a dataloader from numpy data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqcZjrBvLDEM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "5819881d-2f7b-41e9-fa6a-11286cf4a4a4"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X_train = np.loadtxt('data/X1_train.csv', delimiter=',')\n",
        "X_test = np.loadtxt('data/X1_test.csv', delimiter=',')\n",
        "y_train = np.loadtxt('data/y1_train.csv', delimiter=',')\n",
        "y_test = np.loadtxt('data/y1_test.csv', delimiter=',')\n",
        "\n",
        "# Plot it to see why is it called two-moon dataset\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train);\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwffw8PAYJngG38PDw2OY8P/lMQBKRZ9KOQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RQPU8gPLDEP",
        "colab_type": "text"
      },
      "source": [
        "Now, let's create a PyTorch `DataLoader`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sdmcx1J4kAlz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "084e9295-5f4a-4fcf-b7ce-76f277ea2baf"
      },
      "source": [
        "X_train[1:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.89013901,  0.5816847 ],\n",
              "       [ 0.64305663, -0.23076991],\n",
              "       [ 2.2593771 ,  0.17801536],\n",
              "       [ 0.70935303, -0.24496862],\n",
              "       [ 2.52702369,  0.79073014],\n",
              "       [-0.1939462 ,  0.17343962],\n",
              "       [ 0.75113042, -0.68284778],\n",
              "       [ 0.52534017,  0.04206378],\n",
              "       [-0.2928505 ,  0.73368353],\n",
              "       [ 0.10902716,  0.76213182],\n",
              "       [ 1.26130092,  0.76238866],\n",
              "       [ 1.55024077, -0.49602774],\n",
              "       [-0.09640985,  0.11684894],\n",
              "       [-0.52536158,  0.23001757],\n",
              "       [-0.06270236,  1.15464036],\n",
              "       [ 1.02358656, -0.21639312],\n",
              "       [-1.25397045,  0.19121425],\n",
              "       [-1.04405948, -0.07309302],\n",
              "       [ 0.12562716,  1.00263207]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_fsEtCDkGT9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8f5c644c-fc39-4a1c-b546-a87352100f7a"
      },
      "source": [
        "y_train[1:20]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
              "       0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ClzQZjCkl7_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "f71f5fd5-3a8e-4cfe-acf1-b1d5befb9e38"
      },
      "source": [
        "train_dataset[1:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 2.8901,  0.5817],\n",
              "         [ 0.6431, -0.2308],\n",
              "         [ 2.2594,  0.1780],\n",
              "         [ 0.7094, -0.2450],\n",
              "         [ 2.5270,  0.7907],\n",
              "         [-0.1939,  0.1734],\n",
              "         [ 0.7511, -0.6828],\n",
              "         [ 0.5253,  0.0421],\n",
              "         [-0.2929,  0.7337]]), tensor([[1],\n",
              "         [1],\n",
              "         [1],\n",
              "         [1],\n",
              "         [1],\n",
              "         [1],\n",
              "         [1],\n",
              "         [1],\n",
              "         [0]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "201CmQ6VLDEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "batch_size = 64 # mini-batch size\n",
        "num_workers = 4 # how many parallel workers are we gonna use for reading data\n",
        "shuffle = True # shuffle the dataset\n",
        "\n",
        "# Convert numpy array import torch tensor\n",
        "X_train = torch.FloatTensor(X_train)\n",
        "X_test = torch.FloatTensor(X_test)\n",
        "y_train = torch.LongTensor(y_train.reshape(-1, 1))\n",
        "y_test = torch.LongTensor(y_test.reshape(-1, 1))\n",
        "\n",
        "# First, create a dataset from torch tensor. A dataset define how to read data\n",
        "# and process data for creating mini-batches.\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
        "                          num_workers=num_workers, shuffle=shuffle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRZFoX0gLDES",
        "colab_type": "text"
      },
      "source": [
        "Below, we provide a simple example on how to train your model with this dataloader:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnnGETw5LDET",
        "colab_type": "code",
        "colab": {},
        "outputId": "9d9249b0-008b-4658-f254-fab3a8b48678"
      },
      "source": [
        "epoch = 5 # an epoch means looping through all the data in the datasets\n",
        "lr = 1e-1\n",
        "\n",
        "# create a simple model that is probably not gonna work well\n",
        "model = nn.Linear(X_train.size(1), 1)\n",
        "optim = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "for e in range(epoch):\n",
        "    loss_epoch = 0\n",
        "    # loop through train loader to get x and y\n",
        "    for x, y in train_loader:\n",
        "        optim.zero_grad()\n",
        "        y_pred = model(x)\n",
        "        # !!WARNING!!\n",
        "        # THIS IS A CLASSIFICATION TASK, SO YOU SHOULD NOT\n",
        "        # USE THIS LOSS FUNCTION. \n",
        "        loss = (y_pred - y.float()).abs().mean()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        loss_epoch += loss.item()\n",
        "    print(f'Epcoh {e}: {loss_epoch}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epcoh 0: 5.814182370901108\n",
            "Epcoh 1: 2.9536602795124054\n",
            "Epcoh 2: 2.906713977456093\n",
            "Epcoh 3: 2.8909642547369003\n",
            "Epcoh 4: 2.8859013468027115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ4q0xlvLDEV",
        "colab_type": "text"
      },
      "source": [
        "### 1.3.1 Your Simple NN [30 pts]\n",
        "Now, it is time for you to implement your own model for this classification task. Your job here is to:\n",
        "1. Complete the SimpleNN class. It should be a 2- or 3-layer NN with proper non-linearity.\n",
        "2. Train your model with SGD optimizer.\n",
        "3. Tune your model a bit so you can achieve at least 80% accuracy on training set.\n",
        "Hint: you might want to look up `nn.ReLU`, `nn.Sigmoid`, `nn.BCELoss` in the [official document](https://pytorch.org/docs/stable/). You are allowed to freely pick the hyperparameters of your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqBNhP_1LDEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        # super().__init__()\n",
        "        ################################################################################\n",
        "        # TODO:                                                                        #\n",
        "        # Construct your small feedforward NN here.                                    #\n",
        "        ################################################################################\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.linear1 = nn.Linear(D_in, H)\n",
        "        self.linear2 = nn.Linear(H, D_out)\n",
        "        \n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "        \n",
        "    def forward(self, x):\n",
        "        ################################################################################\n",
        "        # TODO:                                                                        #\n",
        "        # feed the input to your network, and output the predictions.                  #\n",
        "        ################################################################################\n",
        "        h_relu = self.linear1(x).clamp(min=0)\n",
        "        y_pred = self.linear2(h_relu)\n",
        "        return y_pred\n",
        "    \n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U85kuZgfLDEa",
        "colab_type": "code",
        "colab": {},
        "outputId": "f2aaee83-701c-45e9-da1d-975cfd6ed09a"
      },
      "source": [
        "# an epoch means looping through all the data in the datasets\n",
        "epoch = 10\n",
        "lr = 1e-1\n",
        "\n",
        "# create a simple model that is probably not gonna work well\n",
        "\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Initialize your model and SGD optimizer here.                                #\n",
        "################################################################################\n",
        "\n",
        "D_in, H, D_out = X_train.size(1), 10, 1\n",
        "\n",
        "model = SimpleNN(D_in, H, D_out)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#                                 END OF YOUR CODE                             #\n",
        "################################################################################\n",
        "\n",
        "\n",
        "for e in range(epoch):\n",
        "    loss_epoch = 0\n",
        "    ################################################################################\n",
        "    # TODO:                                                                        #\n",
        "    # Loop through the dataloader and train your model with nn.BCELoss.            #\n",
        "    ################################################################################\n",
        "    \n",
        "    for x, y in train_loader:\n",
        "        y_pred = model.forward(x)\n",
        "        loss = (y_pred - y.float()).abs().mean()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_epoch += loss.item()\n",
        "    print(f'Epcoh {e}: {loss_epoch}')\n",
        "    \n",
        "    ################################################################################\n",
        "    #                                 END OF YOUR CODE                             #\n",
        "    ################################################################################"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epcoh 0: 2.1046195328235626\n",
            "Epcoh 1: 1.6465306729078293\n",
            "Epcoh 2: 1.533525288105011\n",
            "Epcoh 3: 1.4887716695666313\n",
            "Epcoh 4: 1.449838064610958\n",
            "Epcoh 5: 1.4225691184401512\n",
            "Epcoh 6: 1.401479311287403\n",
            "Epcoh 7: 1.40322595089674\n",
            "Epcoh 8: 1.41878142952919\n",
            "Epcoh 9: 1.3780907317996025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlgaghBJLDEe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper function for computing accuracy\n",
        "def get_acc(pred, y):\n",
        "    pred = pred.float()\n",
        "    y = y.float()\n",
        "    return (y==pred).sum().float()/y.size(0)*100."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG3k-mUuLDEg",
        "colab_type": "text"
      },
      "source": [
        "Evaluate your accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ab-hakCLDEh",
        "colab_type": "code",
        "colab": {},
        "outputId": "990e7e71-8990-44a3-df1d-1ace29989e82"
      },
      "source": [
        "y_pred = (model.forward(X_train) > 0.5)\n",
        "train_acc = get_acc(y_pred, y_train)\n",
        "\n",
        "y_pred = (model.forward(X_test) > 0.5)\n",
        "test_acc = get_acc(y_pred, y_test)\n",
        "print(f'Training accuracy: {train_acc}, Testing accuracy: {test_acc}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy: 84.28571319580078, Testing accuracy: 83.33332824707031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TbI2C3RLDEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Summary: \n",
        "# Train: 700; Test: 300\n",
        "# Input: 2 (ex: [-0.7121,  0.6950])     Output: 1 (ex: 0 or 1)\n",
        "# Architecture: 2-layers NN with 10 hidden nodes\n",
        "# Activation function: ReLU\n",
        "# Optimization: SGD \n",
        "# Loss function: MSELoss\n",
        "# Hyperparameter: epoch = 10,learning rate: lr = 1e-1\n",
        "# This simple NN acheives over 80% accuracy for both train and test dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okRug6M1LDEm",
        "colab_type": "text"
      },
      "source": [
        "# Section 2. Image Classification with CNN [70 pts]\n",
        "Now, we are back to the image classification problem. In this section, our goal is to, again, train models on CIFAR-10 to perform image classification. Your tasks here are to:\n",
        "1. Build and Train a simple feed-forward Neural Network (consists of only nn.Linear layer with activation function) for the classification task\n",
        "2. Build and Train a **Convolutional** Neural Network (CNN) for the classification task\n",
        "3. Try different settings for training your CNN\n",
        "4. Reproduce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6KdjjmzLDEm",
        "colab_type": "text"
      },
      "source": [
        "In the following cell, we provide the code for creating a CIFAR10 dataloader. As you can see, PyTorch's `torchvision` package actually has an interface for the CIFAR10 dataset: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paNskxgVLDEn",
        "colab_type": "code",
        "colab": {},
        "outputId": "53714e5e-88e7-487e-84a7-0436c6dc4e97"
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Preprocessing steps on the training/testing data. You can define your own data augmentation\n",
        "# here, and PyTorch's API will do the rest for you.\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# This will automatically download the dataset for you if it cannot find the data in root\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTBZrDaALDEq",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 Simple NN [30 pts]\n",
        "Implement a simple feed-forward neural network, and train it on the CIFAR-10 training set. Here's some specific requirements:\n",
        "1. The network should only consists of `nn.Linear` layers and the activation functions of your choices (e.g. `nn.Tanh`, `nn.ReLU`, `nn.Sigmoid`, etc). \n",
        "2. Train your model with `torch.optim.SGD` with the hyperparameters you like the most. \n",
        "\n",
        "Note that the hyperparameters work in previous assignment might not work the same, as the implementations of layers could be different."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQeETiVELDEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        super().__init__()\n",
        "        ################################################################################\n",
        "        # TODO:                                                                        #\n",
        "        # Construct your small feedforward NN here.                                    #\n",
        "        ################################################################################\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.linear1 = nn.Linear(D_in, H)\n",
        "        self.linear2 = nn.Linear(H, D_out)\n",
        "        \n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # note that: here, the data is of the shape (B, C, H, W)\n",
        "        # where B is the batch size, C is color channels, and H\n",
        "        # and W is height and width.\n",
        "        # To feed it into the linear layer, we need to reshape it\n",
        "        # with .view() function.\n",
        "        batch_size = x.size(0)\n",
        "        x = x.view(batch_size, -1) # reshape the data from (B, C, H, W) to (B, C*H*W)\n",
        "        ################################################################################\n",
        "        # TODO:                                                                        #\n",
        "        # Forward pass, output the prediction score.                                   #\n",
        "        ################################################################################\n",
        "        h_relu = self.linear1(x).clamp(min=0)\n",
        "        y_pred = self.linear2(h_relu)\n",
        "        return y_pred\n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YV0F4qQNLDEy",
        "colab_type": "code",
        "colab": {},
        "outputId": "dbbbe635-5e0d-49a2-9c50-3de82990b462"
      },
      "source": [
        "epoch = 10\n",
        "lr = 1e-2\n",
        "n_input = 3072\n",
        "n_classes = 10\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Your training code here.                                                     #\n",
        "################################################################################\n",
        "\n",
        "H = 100\n",
        "model = SimpleNN(n_input, H, n_classes)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "for e in range(epoch):\n",
        "    loss_epoch = 0\n",
        "    for x, y in train_loader:\n",
        "        y_pred = model.forward(x)\n",
        "        loss = loss_fn(y_pred,y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_epoch += loss.item()\n",
        "    print(f'Epcoh {e}: {loss_epoch}')\n",
        "\n",
        "\n",
        "################################################################################\n",
        "#                                 END OF YOUR CODE                             #\n",
        "################################################################################\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epcoh 0: 1354.4918031692505\n",
            "Epcoh 1: 1208.396605372429\n",
            "Epcoh 2: 1145.9103227853775\n",
            "Epcoh 3: 1099.5618304014206\n",
            "Epcoh 4: 1063.9656347036362\n",
            "Epcoh 5: 1034.5730928182602\n",
            "Epcoh 6: 1007.9670815467834\n",
            "Epcoh 7: 984.9500880241394\n",
            "Epcoh 8: 964.326208293438\n",
            "Epcoh 9: 946.5294954776764\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8PHkIsBLDE0",
        "colab_type": "text"
      },
      "source": [
        "Now evaluate your model with the helper function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtHe6PfxLDE0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model_acc(model, loader):\n",
        "    ys = []\n",
        "    y_preds = []\n",
        "    for x, y in loader:\n",
        "        ys.append(y)\n",
        "        # set the prediction to the one that has highest value\n",
        "        # Note that the the output size of model(x) is (B, 10)\n",
        "        y_preds.append(torch.argmax(model(x), dim=1))\n",
        "    y = torch.cat(ys, dim=0)\n",
        "    y_pred = torch.cat(y_preds, dim=0)\n",
        "    print((y == y_pred).sum())\n",
        "    return get_acc(y_pred, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KID4IicZLDE3",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.1 Evaluate NN [30 pts]\n",
        "Evaluate your NN. You should get an accuracy around **50%** on training set and **49%** on testing set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88AOo-qDLDE3",
        "colab_type": "code",
        "colab": {},
        "outputId": "12d02063-0136-46ed-cc34-3ce5d1b6a58d"
      },
      "source": [
        "train_acc = get_model_acc(model, train_loader)\n",
        "test_acc = get_model_acc(model, test_loader)\n",
        "print(f'Training accuracy: {train_acc}, Testing accuracy: {test_acc}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(28675)\n",
            "tensor(4919)\n",
            "Training accuracy: 57.349998474121094, Testing accuracy: 49.189998626708984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE6KQC4GLDE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Summary: \n",
        "# Input: 3072      Output: 10\n",
        "# Architecture: 2-layers NN with 100 hidden nodes\n",
        "# Activation function: ReLU\n",
        "# Optimization: SGD \n",
        "# Loss function: CrossEntropyLoss\n",
        "# Hyperparameter: epoch = 10,learning rate: lr = 1e-1; batch size: 64\n",
        "# This simple NN acheives accuracy around 50% on training set and 49% on testing set."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-srm1GG0LDE9",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Convolutional Neural Network (CNN) [40 pts]\n",
        "Convolutional layer has been proven to be extremely useful for vision-based task. As mentioned in the lecture, this speical layer allows the model to learn filters that capture crucial visual features. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ3yu0sqLDE-",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.1 Implement and Evaluate CNN [10 pts]\n",
        "In this section, you will need to construct a CNN for classifying CIFAR-10 image. Specifically, you need to:\n",
        "1. build a `CNNClassifier` with `nn.Conv2d`, `nn.Maxpool2d` and activation functions that you think are appropriate. \n",
        "2. You would need to flatten the output of your convolutional networks with `view()`, and feed it into a `nn.Linear` layer to predict the class labels of the input. \n",
        "\n",
        "Once you are done with your module, train it with `optim.SGD`, and evaluate it. You should get an accuracy around **55%** on training set and **53%** on testing set.\n",
        "\n",
        "Hint: You might want to look up `nn.Conv2d`, `nn.Maxpool2d`, `nn.CrossEntropyLoss()`, `view()` and `size()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2Bl64B_LDE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class CNNClassifier(nn.Module):\n",
        "    \n",
        "    def __init__(self,in_channel, out_channel, filter_size):\n",
        "        ################################################################################\n",
        "        # TODO:                                                                        #\n",
        "        # Construct a CNN with 2 or 3 convolutional layers and 1 linear layer for      #\n",
        "        # outputing class prediction. You are free to pick the hyperparameters         #\n",
        "        ################################################################################\n",
        "        \n",
        "        super(CNNClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channel, 15, filter_size)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(15, 12, filter_size)\n",
        "        self.linear1=nn.Linear(12 * 10 * 10, out_channel)\n",
        "        \n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        ################################################################################\n",
        "        # TODO:                                                                        #\n",
        "        # Forward pass of your network. First extract feature with CNN, and predict    #\n",
        "        # class scores with linear layer. Be careful about your input/output shape.    #\n",
        "        ################################################################################\n",
        "        \n",
        "        # print(x.size())\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = x.view(-1, 12* 10 * 10)\n",
        "        return self.linear1(x)\n",
        "        \n",
        "        ################################################################################\n",
        "        #                                 END OF YOUR CODE                             #\n",
        "        ################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "hFDWIJD2LDFB",
        "colab_type": "code",
        "colab": {},
        "outputId": "e1e7485b-6e0a-4173-a2e6-4b2ca0ca15a3"
      },
      "source": [
        "# You can tune these hyperparameters as you like.\n",
        "epoch = 10\n",
        "lr = 1e-2\n",
        "n_input = 3072\n",
        "n_classes = 10\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Your training code here.                                                     #\n",
        "################################################################################\n",
        "\n",
        "model = CNNClassifier(3,10,5)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for e in range(epoch):\n",
        "    loss_epoch = 0\n",
        "    for x, y in train_loader:\n",
        "        y_pred = model.forward(x)\n",
        "        loss = loss_fn(y_pred,y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_epoch += loss.item()\n",
        "    print(f'Epcoh {e}: {loss_epoch}')\n",
        "    \n",
        "\n",
        "################################################################################\n",
        "#                                 END OF YOUR CODE                             #\n",
        "################################################################################"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epcoh 0: 1327.052695274353\n",
            "Epcoh 1: 1086.5805433988571\n",
            "Epcoh 2: 1017.3189124464989\n",
            "Epcoh 3: 961.9801934957504\n",
            "Epcoh 4: 913.0858766436577\n",
            "Epcoh 5: 876.5575193762779\n",
            "Epcoh 6: 853.1251313686371\n",
            "Epcoh 7: 836.4923467040062\n",
            "Epcoh 8: 822.2498172521591\n",
            "Epcoh 9: 809.7935128211975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT89n3ADLDFD",
        "colab_type": "code",
        "colab": {},
        "outputId": "f309f5e6-1701-4cf6-ac7c-2bb09573efea"
      },
      "source": [
        "# turn on evaluation mode. This is crucial when you have BatchNorm in your network,\n",
        "# as you want to use the running mean/std you obtain durining training time to normalize\n",
        "# your input data. Rememeber to call .train() function after evaluation\n",
        "model.eval()\n",
        "train_acc = get_model_acc(model, train_loader)\n",
        "test_acc = get_model_acc(model, test_loader)\n",
        "print(f'Training accuracy: {train_acc}, Testing accuracy: {test_acc}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(31076)\n",
            "tensor(5970)\n",
            "Training accuracy: 62.15199661254883, Testing accuracy: 59.70000076293945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqD40QfQLDFF",
        "colab_type": "text"
      },
      "source": [
        "<span style=\"color:red\">**Explain your design and hyperparameter choice in three or four sentences:**</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1115lTtRLDFG",
        "colab_type": "text"
      },
      "source": [
        "Design: \n",
        "    The network contains 2 convolutional layers and 1 max pooling layer. \n",
        "    The input channel is 3; Middle channel is 15; Output channel is 10;\n",
        "    Filter size is 5. Stride=1. Pooling size is 2. No padding.\n",
        "    Optimization: SGD\n",
        "    Loss function: CrossEntropyLoss\n",
        "    \n",
        "Hyperparameter:\n",
        "    epoch = 10\n",
        "    lr = 1e-2\n",
        "    batch_size = 64\n",
        "    \n",
        "Results:\n",
        "    This simple CNN acheives accuracy around 62% on training set and 59% on testing set. \n",
        "    I made this chocie because it can give me best accuracy by trail and error. \n",
        "    Also, the CNN is simple enough for my laptop to run. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STCCrPtDLDFH",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.2 STACK MORE LAYERS [10 pts]\n",
        "Now, **try at least 4 network architectures with different numbers of convolutional layers**. Train these settings with `optim.SGD`, plot the accuracy as a fuction of convolutional layers and describe what you have observed (running time, performance, etc). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zMYLt3ILDFI",
        "colab_type": "code",
        "colab": {},
        "outputId": "3c547b2f-2ef1-419c-f5c0-18c85196e452"
      },
      "source": [
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Your training code here.                                                     #\n",
        "################################################################################\n",
        "import torch.nn.functional as F\n",
        "## 1-layer CNN\n",
        "%time\n",
        "class CNNLayer1(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels):\n",
        "        super(CNNLayer1,self).__init__()\n",
        "        #Create 1st layer\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels,out_channels=8,kernel_size=5)\n",
        "        self.linear = nn.Linear(in_features=8*28*28,out_features=out_channels)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = x.view(-1,8*28*28)\n",
        "        return self.linear(x)\n",
        "\n",
        "##  2-layer CNN\n",
        "class CNNLayer2(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels):\n",
        "        super(CNNLayer2,self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels,out_channels=8,kernel_size=5)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=8,out_channels=8,kernel_size=5)\n",
        "        self.linear = nn.Linear(in_features=8*10*10,out_features=out_channels)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = x.view(-1,8*10*10)\n",
        "        return self.linear(x)\n",
        "\n",
        "##  3-layer CNN\n",
        "class CNNLayer3(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels):\n",
        "        super(CNNLayer3,self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels,out_channels=8,kernel_size=5)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=8,out_channels=8,kernel_size=5)\n",
        "        self.conv3 = nn.Conv2d(in_channels=8,out_channels=8,kernel_size=5)\n",
        "        self.linear = nn.Linear(in_features=8*6*6,out_features=out_channels)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.conv3(x)\n",
        "        x = x.view(-1,8*6*6)\n",
        "        return self.linear(x)\n",
        "\n",
        "##  4-layer CNN\n",
        "class CNNLayer4(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels):\n",
        "        super(CNNLayer4,self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels,out_channels=8,kernel_size=5)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=8,out_channels=8,kernel_size=5)\n",
        "        self.conv3 = nn.Conv2d(in_channels=8,out_channels=8,kernel_size=5)\n",
        "        self.conv4 = nn.Conv2d(in_channels=8,out_channels=8,kernel_size=5)\n",
        "        self.linear = nn.Linear(in_features=8*2*2,out_features=out_channels)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.conv4(x)\n",
        "        x = x.view(-1,8*2*2)\n",
        "        return self.linear(x)\n",
        "\n",
        "# Train the CNN \n",
        "epoch = 10\n",
        "lr = 1e-2\n",
        "n_input = 3072\n",
        "n_classes = 10\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "model = CNNLayer4(3,10)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "for e in range(epoch):\n",
        "    loss_epoch = 0\n",
        "    for x, y in train_loader:\n",
        "        y_pred = model.forward(x)\n",
        "        loss = loss_fn(y_pred,y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_epoch += loss.item()\n",
        "    print(f'Epcoh {e}: {loss_epoch}')\n",
        "\n",
        "model.eval()\n",
        "train_acc = get_model_acc(model, train_loader)\n",
        "test_acc = get_model_acc(model, test_loader)\n",
        "print(f'Training accuracy: {train_acc}, Testing accuracy: {test_acc}')\n",
        "\n",
        "    \n",
        "################################################################################\n",
        "#                                 END OF YOUR CODE                             #\n",
        "################################################################################"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3 s, sys: 1e+03 ns, total: 4 s\n",
            "Wall time: 5.96 s\n",
            "Epcoh 0: 1688.572285413742\n",
            "Epcoh 1: 1465.2414149045944\n",
            "Epcoh 2: 1329.496152639389\n",
            "Epcoh 3: 1235.3316926956177\n",
            "Epcoh 4: 1171.414178609848\n",
            "Epcoh 5: 1134.991012096405\n",
            "Epcoh 6: 1109.8472093343735\n",
            "Epcoh 7: 1088.250674366951\n",
            "Epcoh 8: 1070.494764149189\n",
            "Epcoh 9: 1057.0194869041443\n",
            "tensor(21749)\n",
            "tensor(4253)\n",
            "Training accuracy: 43.49800109863281, Testing accuracy: 42.529998779296875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Gq4nVnALDFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "1-layer CNN Output: \n",
        "Epcoh 0: 1406.8942036628723\n",
        "Epcoh 1: 1355.0191254615784\n",
        "Epcoh 2: 1339.4922153949738\n",
        "Epcoh 3: 1330.805212855339\n",
        "Epcoh 4: 1322.5252001285553\n",
        "Epcoh 5: 1316.581095457077\n",
        "Epcoh 6: 1310.009941458702\n",
        "Epcoh 7: 1306.171748161316\n",
        "Epcoh 8: 1300.480077624321\n",
        "Epcoh 9: 1297.2553458213806\n",
        "tensor(21127)\n",
        "tensor(3712)\n",
        "Training accuracy: 42.25400161743164, Testing accuracy: 37.119998931884766\n",
        "\n",
        "2-layer CNN Output: \n",
        "Epcoh 0: 1373.0496850013733\n",
        "Epcoh 1: 1151.033370733261\n",
        "Epcoh 2: 1078.3630838990211\n",
        "Epcoh 3: 1029.7335131168365\n",
        "Epcoh 4: 993.3573430776596\n",
        "Epcoh 5: 970.7726957201958\n",
        "Epcoh 6: 949.9841889739037\n",
        "Epcoh 7: 932.6380230784416\n",
        "Epcoh 8: 919.5890220403671\n",
        "Epcoh 9: 908.4062338471413\n",
        "tensor(29063)\n",
        "tensor(5608)\n",
        "Training accuracy: 58.12600326538086, Testing accuracy: 56.08000183105469\n",
        "        \n",
        "3-layer CNN Output:\n",
        "Epcoh 0: 1563.753846168518\n",
        "Epcoh 1: 1286.564388513565\n",
        "Epcoh 2: 1192.53229367733\n",
        "Epcoh 3: 1132.1954835653305\n",
        "Epcoh 4: 1090.5051774382591\n",
        "Epcoh 5: 1052.856717467308\n",
        "Epcoh 6: 1027.9334802031517\n",
        "Epcoh 7: 1007.6736091971397\n",
        "Epcoh 8: 991.900506913662\n",
        "Epcoh 9: 978.280636370182\n",
        "tensor(27850)\n",
        "tensor(5453)\n",
        "Training accuracy: 55.69999694824219, Testing accuracy: 54.529998779296875\n",
        "        \n",
        "4-layer CNN Output:\n",
        "Epcoh 0: 1688.572285413742\n",
        "Epcoh 1: 1465.2414149045944\n",
        "Epcoh 2: 1329.496152639389\n",
        "Epcoh 3: 1235.3316926956177\n",
        "Epcoh 4: 1171.414178609848\n",
        "Epcoh 5: 1134.991012096405\n",
        "Epcoh 6: 1109.8472093343735\n",
        "Epcoh 7: 1088.250674366951\n",
        "Epcoh 8: 1070.494764149189\n",
        "Epcoh 9: 1057.0194869041443\n",
        "tensor(21749)\n",
        "tensor(4253)\n",
        "Training accuracy: 43.49800109863281, Testing accuracy: 42.529998779296875"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWAlLjAyLDFQ",
        "colab_type": "code",
        "colab": {},
        "outputId": "8632d083-dc8f-411b-c04e-4b6409abddd3"
      },
      "source": [
        "# Plot the result: \n",
        "layer_list=[1,2,3,4]\n",
        "acu_list=[37.119998931884766,56.08000183105469,54.529998779296875,42.529998779296875]\n",
        "plt.plot(layer_list,acu_list,'bo');\n",
        "plt.ylabel('Acuracy of Test');\n",
        "plt.xlabel('Number of Layers');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CwsJfo0LDFS",
        "colab_type": "text"
      },
      "source": [
        "<span style=\"color:red\">**Briefly explain what you have observed in three or four sentences. Does stacking layers always give you better results? How about the computational time?:**</span>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEv_s6l4LDFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "The accuracy increases at first and then decreases when we increase the number of layers. \n",
        "It does not always give better result by stacking more layers.\n",
        "We find that it takes more computational time to train CNN with more layers. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K4JlE3FLDFV",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.3 Optimizer? Optimizer! [10 pts]\n",
        "So far, we only use SGD as our optimizer. Now, pick two other optimizers, train your CNN models, and compare the performance you get. What did you see?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKTc9ewULDFV",
        "colab_type": "code",
        "colab": {},
        "outputId": "6b60f2fd-87d0-48d5-aab9-a3557414b611"
      },
      "source": [
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Your training code here.                                                     #\n",
        "################################################################################\n",
        "\n",
        "import torch.optim as Opt\n",
        "# You can tune these hyperparameters as you like.\n",
        "epoch = 10\n",
        "lr = 1e-2\n",
        "n_input = 3072\n",
        "n_classes = 10\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "model = CNNLayer2(3,10)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# choose different optimizers:\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "optimizer = torch.optim.Adadelta(model.parameters(), lr=lr)\n",
        "\n",
        "for e in range(epoch):\n",
        "    loss_epoch = 0\n",
        "    for x, y in train_loader:\n",
        "        y_pred = model.forward(x)\n",
        "        loss = loss_fn(y_pred,y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_epoch += loss.item()\n",
        "    print(f'Epcoh {e}: {loss_epoch}')\n",
        "    \n",
        "model.eval()\n",
        "train_acc = get_model_acc(model, train_loader)\n",
        "test_acc = get_model_acc(model, test_loader)\n",
        "print(f'Training accuracy: {train_acc}, Testing accuracy: {test_acc}')\n",
        "\n",
        "################################################################################\n",
        "#                                 END OF YOUR CODE                             #\n",
        "################################################################################"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epcoh 0: 1646.5864316225052\n",
            "Epcoh 1: 1508.3467164039612\n",
            "Epcoh 2: 1446.6222896575928\n",
            "Epcoh 3: 1390.4752886295319\n",
            "Epcoh 4: 1336.2126928567886\n",
            "Epcoh 5: 1288.947086572647\n",
            "Epcoh 6: 1251.0049463510513\n",
            "Epcoh 7: 1222.1448757648468\n",
            "Epcoh 8: 1198.5986899137497\n",
            "Epcoh 9: 1179.6359493732452\n",
            "tensor(23476)\n",
            "tensor(4713)\n",
            "Training accuracy: 46.95199966430664, Testing accuracy: 47.130001068115234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdAl2kKXLDFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SDG output: \n",
        "Epcoh 0: 1373.0496850013733\n",
        "Epcoh 1: 1151.033370733261\n",
        "Epcoh 2: 1078.3630838990211\n",
        "Epcoh 3: 1029.7335131168365\n",
        "Epcoh 4: 993.3573430776596\n",
        "Epcoh 5: 970.7726957201958\n",
        "Epcoh 6: 949.9841889739037\n",
        "Epcoh 7: 932.6380230784416\n",
        "Epcoh 8: 919.5890220403671\n",
        "Epcoh 9: 908.4062338471413\n",
        "tensor(29063)\n",
        "tensor(5608)\n",
        "Training accuracy: 58.12600326538086, Testing accuracy: 56.08000183105469\n",
        "\n",
        "Adam output:\n",
        "Epcoh 0: 1411.3866897821426\n",
        "Epcoh 1: 1331.034428358078\n",
        "Epcoh 2: 1373.3354017734528\n",
        "Epcoh 3: 1349.5545563697815\n",
        "Epcoh 4: 1343.59157538414\n",
        "Epcoh 5: 1359.3811277151108\n",
        "Epcoh 6: 1352.0158956050873\n",
        "Epcoh 7: 1347.3744359016418\n",
        "Epcoh 8: 1360.8486785888672\n",
        "Epcoh 9: 1332.8903744220734\n",
        "tensor(19177)\n",
        "tensor(3639)\n",
        "Training accuracy: 38.354000091552734, Testing accuracy: 36.38999938964844\n",
        "        \n",
        "Adadelta output: \n",
        "Epcoh 0: 1646.5864316225052\n",
        "Epcoh 1: 1508.3467164039612\n",
        "Epcoh 2: 1446.6222896575928\n",
        "Epcoh 3: 1390.4752886295319\n",
        "Epcoh 4: 1336.2126928567886\n",
        "Epcoh 5: 1288.947086572647\n",
        "Epcoh 6: 1251.0049463510513\n",
        "Epcoh 7: 1222.1448757648468\n",
        "Epcoh 8: 1198.5986899137497\n",
        "Epcoh 9: 1179.6359493732452\n",
        "tensor(23476)\n",
        "tensor(4713)\n",
        "Training accuracy: 46.95199966430664, Testing accuracy: 47.130001068115234"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs2ptNBcLDFZ",
        "colab_type": "text"
      },
      "source": [
        "<span style=\"color:red\">**What did you see? Which optimizer is your favorite? Describe:**</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT0uqjQDLDFa",
        "colab_type": "text"
      },
      "source": [
        "I train the same CNN with three different optimization algorithms, SGD, Adam and Adadelta. \n",
        "From our results, SGD performs best, then Adadelta and Adam. \n",
        "My favorite one is SGD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9Uq7CidLDFb",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.4 Improve Your Model [10 pts]\n",
        "Again, we want you to play with your model a bit harder, and improve it. You are free to use everything you can find in the documents (`BatchNorm`, `SeLU`, etc), as long as it is not a **predefined network architectures in PyTorch package**. You can also implement some famous network architectures to push the performance. \n",
        "\n",
        "(A simple network with 5-6 `nn.Conv2d` can give you at least 70% accuracy on testing set)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOl56_T7LDFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Your training code here.                                                     #\n",
        "################################################################################\n",
        "\n",
        "class ConvBatSeLu(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels):\n",
        "        super(ConvBatSeLu,self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=5)\n",
        "        self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
        "\n",
        "    def forward(self,input):\n",
        "        output = self.conv(input)\n",
        "        output = self.bn(output)\n",
        "        output = F.selu(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "class MyBestCNN(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels):\n",
        "        super(MyBestCNN,self).__init__()\n",
        "        self.conv1 = ConvBatSeLu(in_channels=in_channels,out_channels=16)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = ConvBatSeLu(in_channels=16,out_channels=16)\n",
        "        self.linear = nn.Linear(in_features=16*10*10,out_features=out_channels)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = x.view(-1,16*10*10)\n",
        "        return self.linear(x)\n",
        "\n",
        "\n",
        "# Train the CNN \n",
        "epoch = 10\n",
        "lr = 1e-2\n",
        "n_input = 3072\n",
        "n_classes = 10\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "model = MyBestCNN(3,10)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "for e in range(epoch):\n",
        "    loss_epoch = 0\n",
        "    for x, y in train_loader:\n",
        "        y_pred = model.forward(x)\n",
        "        loss = loss_fn(y_pred,y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_epoch += loss.item()\n",
        "    print(f'Epcoh {e}: {loss_epoch}')\n",
        "\n",
        "model.eval()\n",
        "train_acc = get_model_acc(model, train_loader)\n",
        "test_acc = get_model_acc(model, test_loader)\n",
        "print(f'Training accuracy: {train_acc}, Testing accuracy: {test_acc}')\n",
        "\n",
        "################################################################################\n",
        "#                                 END OF YOUR CODE                             #\n",
        "################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eM-GIKULDFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The result:\n",
        "\n",
        "Epcoh 0: 1184.8225207328796\n",
        "Epcoh 1: 963.2583043575287\n",
        "Epcoh 2: 879.5836268663406\n",
        "Epcoh 3: 831.4896352291107\n",
        "Epcoh 4: 790.4476766586304\n",
        "Epcoh 5: 761.3781149983406\n",
        "Epcoh 6: 736.0012830495834\n",
        "Epcoh 7: 717.3178885877132\n",
        "Epcoh 8: 701.5758913755417\n",
        "Epcoh 9: 686.2442553639412\n",
        "tensor(33978)\n",
        "tensor(6492)\n",
        "Training accuracy: 67.95600128173828, Testing accuracy: 64.92000579833984"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTiQAmH3LDFp",
        "colab_type": "text"
      },
      "source": [
        "The CNN has 2 layers.The design and hyperparameter is the same with previous CNN, which has 55% accuracy on test dataset. Here, we make improvement by introducing batchnorm layer before each convolutional layers. Keeping all other things unchanged,we get a result with accuracy around 65% on test dataset. "
      ]
    }
  ]
}